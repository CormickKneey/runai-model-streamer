{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stream Model To CPU\n",
    "\n",
    "In this notebook we will demonstrate how to read tensors using the RunAI Model Streamer to the CPU memory and perform computation on their data.\n",
    "\n",
    "The only requirement for running this notebook is a Linux machine.\n",
    "\n",
    "## Preperation\n",
    "We will start by downloading an example `.safetensors` file. Feel free to use your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "url = 'https://huggingface.co/vidore/colpali/resolve/main/adapter_model.safetensors?download=true'\n",
    "local_filename = 'model.safetensors'\n",
    "\n",
    "wget_command = ['wget', '--content-disposition', url, '-O', local_filename]\n",
    "subprocess.run(wget_command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "\n",
    "To load the tensors from the file we need to create `SafetensorsStreamer` instance, perform the request, and start iterating over the tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from runai_model_streamer import SafetensorsStreamer\n",
    "\n",
    "file_path = \"model.safetensors\"\n",
    "\n",
    "def heavy_workload(tensor):\n",
    "    # Perform heavy computation with the tensor\n",
    "    # The computation is performed while the streamer \n",
    "    # continues reading the next tensors in the file\n",
    "    return\n",
    "\n",
    "with SafetensorsStreamer() as streamer:\n",
    "    streamer.stream_file(file_path)\n",
    "    for name, tensor in streamer.get_tensors():\n",
    "        heavy_workload(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A heavy workload can be running on each tensor in the moment the tensor is yielded - in parallel to the CPP threads that continue reading from the storage. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
